\documentclass[12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{aleks}
\author{Magdalena Leshtanska \and Aleksandar Dimitrov}
\title{Error-Centric Annotation of Learner Corpora}
\begin{document}
\maketitle
\tableofcontents
\abstract{I am an abstract!}
\section{Introduction}\label{sec:intro}

Literature overview. Bla Bla.

All of the annotation schemes mentioned above focus on the \textbf{text} itself,
and will often even advise the annotator to \textit{modify} the underlying text
with the annotation. We believe that a more error-centric annotation of the data
can be beneficial.

Firstly, every annotation scheme that operates directly on the text or spans
over a piece of text will run into two kinds of problems:

\begin{itemize}
\item \textit{Interleaving annotations} occur when an error doesn't end before
another one begins. Given the tokens $a_1 a_2 a_3$, and two errors $e_1$ and
$e_2$ ranging over tokens $a_1$, $a_2$ and $a_2$ $a_3$ respectively, the
resulting markup will be confusing or outright impossible to read: $(e_1)a_1
(e_2)a_2(/e_1)a_3(/e_2)$. This is particularly a problem with XML,
since the specification\footnotemark explicitly disallows interleaving
markup.
\item \textit{Greedy annotation} covers tokens entirely uninvolved in the
\textit{``production''} of an error. If of the tokens $a_1 a_2 a_3$ only $a_1$
and $a_3$ are involved in the production of an error (say, an agreement error)
marking the entire token sequence as erroneous would involve blaming the
otherwise completely innocent token $a_2$. This can significantly increase the
noise in a corpus.
\end{itemize}
\footnotetext{Located at \texttt{http://www.w3.org/TR/REC-xml/}}

There is little data about the primary use cases for learner corpora known to
us, but it does not seem too far fetched to assume most linguists interested in
such data are not likely to browse through the corpora in order to find an
amusingly written piece of text. Instead, it seems to be safe to assume that
most would be interested in the \textbf{kinds of errors} that can occur in such
a corpus, and their particular properties. The error data should thus contain as
little noise as possible and be maximally specific while retaining a large amount
of generality.  It should furthermore be easily accessible, without having to
actually read the texts within the corpus itself.

Based on these assumptions, we decided to decouple the \textit{error markup}
from the \textit{corpus data}. Specifically, to our annotation scheme the
errors and the text are two entirely different data structures. Every
particular error can reference tokens within the corpus using a \textbf{key},
similar to the way modern Relational Data Bases reference their data. This makes
the index more accessible and easier to maintain.

\section{Error Taxonomy and Annotation Scheme}\label{sec:scheme}

Learner errors are not easily classified due to their wide variety of
appearance, kind, and ambiguities involved. Typically, an annotation scheme will
strive to cover as large a scope it possibly can unambiguously classify, yet
there are many practical limitations to the potential coverage one can achieve.

One of the most prevalent restrictions is the need to devise the annotation
manual with great attention to detail, and to perform a meaningful assessment of
the quality of the annotated data after the annotation process. Even a most
meticulously well-defined manual is still going to be read and applied by
humans, who will have their personal intuition. It is easy to see
why this inherent individuality poses problems to the consistency of any kind of
annotated data. A well-written annotation manual will limit the effect this
natural variance can have on the data.

\subsection{Calling Errors by Name}\label{sec:taxonomy}

In order to identify errors occurring in learner language, one has to first
classify them. However, even then, identifying which class a particular error
belongs to is in no way an easy job. Devising an exhaustive taxonomy of errors
that can appear in natural language seems a daunting task, since science has so
far failed to regularize what \textit{is} a valid utterance of a language.
Therefore, our taxonomy will try to do some things only, and do them well. In
particular, we will not cover punctuation mistakes, since their possible
corrections can have cascading effects on the rest of the sentences and suddenly
give rise to all kinds of new errors.

Our annotation scheme presents three distinct kinds of errors.

\begin{enumerate}
\item \textit{Spelling errors} are a primitive form of error. They contain no
meta-data or any kind of further specification except maybe a target word they
should have ended up as.
\item \textit{Context errors} are otherwise grammatical sentences which are,
however, semantically incorrect given the current context.
\item \textit{Grammar errors} simply do not belong to the target language because
of their erroneous morphology or syntax.
\end{enumerate}

\subsection{Representing Errors in Annotations}\label{sec:errorformat}

An error annotation is a tuple $\pair{E,C,\theta,t,c}$, where
\begin{itemize}
  \item $E$ is a nonempty set of indices of erroneous tokens,
  \item $C$ is a possibly empty set of context tokens 
  \item $\theta$ is the type of the error,
  \item $s$ a string denoting an optional target hypothesis hint, and
  \item $c$ an optional comment.
\end{itemize}

The error type $\theta$ is defined as an ordered sequence of categories from the
taxonomy presented in \ref{sec:taxonomy}.

We hope this general format to be sufficient for annotation within our current
goals, and extensible enough to cover future refinements of the annotation
standard, such as the addition of part of speech tags or syntactic markup.
Keeping the error annotation in a separate data structure, allows for the
annotation itself to be more detailed and flexible.

\section{Annotation Manual}\label{sec:manual}

\subsection{Spelling Errors}\label{sec:speling}
A spelling error is an error that \textit{cannot} be explained by mistakes in
morphology, e.g. derivation or inflection. It will typically comprise only one
word. If it comprises more than one word, it is seen as a \textit{compound}
spelling error, i.e. two tokens separated by a space should have been
concatenated to yield only one token. Similarly, if the target hypothesis
accounts for more than one word, the token should have been split up into two.

\subsection{Context Errors}
Context errors occur in otherwise completely grammatical sentences, which, however
do not convey what was obviously intended by the author taking context into
account. Lexical errors are also context errors, if the given word is grammatically
valid within the sentence.

\subsection{Grammatical Errors}

\subsection{Devising Target Hypotheses}
Retain existing input by the author of the text to the greatest extent possible.
The text should be only minimally altered in the target hypothesis.

\section{Conclusion}\label{sec:results}

Statistical inter-annotator agreement  measures are a common quality assessment
method used in corpus linguistics and related fields. Hereby, annotations made
on a particular data set by two or more annotators are compared using
quantitative methods.  \cite{ap2008} give an overview of currently employed
methods.

While inter-annotator agreement measures have been applied successfully to
various corpus linguistic tasks, so far they have not found wide usage among
learner language annotation. In fact, we believe the current techniques  are not
applicable to this particular problem domain.

\subsection{Unitization and Multidimensional Markup}

Existing inter-annotator agreement measures %such as â€¦ \cite FIXME all assume
the presence of atomic units in the corpus data, which are annotated by all
annotators of a certain data set. This is the case with part of speech tagging
or syntactic classification of chunked sentences, where all annotators are
presented with a set of tokens or a set of chunks, respectively, which they have
to annotate.

The annotations \textit{over these units} are then used to calculate an
agreement coefficient that will usually range from -1 to 1. However, learner
language is a lot more diverse in nature, and it is not as easily possible to
partition it into atomic data units. Instead, annotators have to \textit{find
data points} and also \textit{manually delineate} them, according to the
annotation manual. This brings the problem of \textit{unitization} into play.

\cite{ap2008} briefly discuss unitization and go on to note that it has thus far
not been exhaustively researched. Even more importantly, they explicitly comment
on the unknown status of the validity of the only inter-annotator agreement
measure in the corpus linguistic literature, $\alpha_U$, presented in
\cite{krip1995}.

Apart from being untested, $\alpha_U$ has several more problems that make it an
ill fit for learner language data. As mentioned in \cite{ap2008}, Krippendorf's
$\alpha_U$ assumes markup to be non-overlapping. This is certainly not the case
with learner language, where one error may be nested within in another
error\footnote{This happens very frequently with spelling errors.}. Also, if a
segment annotated by one annotator spans more than one segment annotated by the
other annotator, $\alpha_U$ will not calculate their agreement correctly.

We see yet another problem with applying $\alpha_U$ to learner corpora: we
showed earlier % FIXME: where
that often learner errors will not present a coherent unit, but may be scattered
across several tokens. $\alpha_U$ does not account for the possibility of
annotations deviating \textit{within} the boundaries of a marked up error.

\subsection{Possible Extensions}

After assessing the quality of our data, we reached
the conclusion that the error format described in \ref{sec:errorformat}
might benefit from several refinements. Adding a field for part of speech tags
might contribute to the clarity of the data, as well as to its searchability.
The annotated corpus could be queried for erroneously placed verbs or
prepositions, for example.

Moreover, we came to the conclusion that defining the error context as a set of
tokens might be misleading or at least difficult to understand in case the error
context does not constitute one sequence, but several scattered sequences, such
as proper nouns or syntactic constituents. $C$ could therefore be turned into a
set of sequences of token indices.

\subsubsection{Towards an underspecification formalism for target hypotheses}

During the annotation process, we discovered that our taxonomy branches for
\textit{omission}, \textit{replacement}, and \textit{redundancy} could be turned
into a stub of a formalism for underspecification of target hypotheses. Instead
of giving a string for a target hypothesis, such a formalism would make it
possible to approximate the target and therefore allow for more flexibility in
the markup. Note, however, that these forms of omission, redundancy and
replacement differ from the ones included in the error taxonomy.

The taxonomy tries to account for \textit{what is wrong} with a given string of
text. A target hypothesis would try to make assumptions about \textit{how this
could be fixed}. Our categories in the error taxonomy suggesting manipulation of
the input text were explicitly designed to catch cases where a clear reason for
an error could not be found, and the syntactic environment of a given set of
tokens would require the text to be changed entirely. Such subcategorization
mistakes could be granted their own category and the target hypothesis could
account for the necessary steps in order to ensure grammaticality.

This would also allow for existing annotations to be combined with
\textit{generic instructions for correcting the input} and increase the
granularity of the data.

\end{document}
